{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6c9bf86-7131-45c6-a3fb-5cc7a6fdb072",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Sample from a trained model\n",
    "\"\"\"\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pickle\n",
    "from contextlib import nullcontext\n",
    "import torch\n",
    "#import tiktoken\n",
    "from model_ukb import GPTConfig, GPT\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn import functional as F\n",
    "\n",
    "os.chdir('/nfs/research/sds/sds-ukb-cancer/projects/gpt/nanoGPT-healthGPT')\n",
    "# -----------------------------------------------------------------------------\n",
    "init_from = 'resume' # either 'resume' (from an out_dir) or a gpt2 variant (e.g. 'gpt2-xl')\n",
    "out_dir = 'out-ukb' # ignored if init_from is not 'resume'\n",
    "start = \"\\n\" # or \"<|endoftext|>\" or etc. Can also specify a file, use as: \"FILE:prompt.txt\"\n",
    "num_samples = 10 # number of samples to draw\n",
    "max_new_tokens = 10 # number of tokens generated in each sample\n",
    "temperature = 0.8 # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n",
    "top_k = 200 # retain only the top_k most likely tokens, clamp others to have 0 probability\n",
    "seed = 1337\n",
    "device = 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1', etc.\n",
    "dtype ='float64'#'bfloat16' # 'float32' or 'bfloat16' or 'float16'\n",
    "compile = False # use PyTorch 2.0 to compile the model to be faster\n",
    "t_min = 100.0\n",
    "#exec(open('configurator.py').read()) # overrides from command line or config file\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
    "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
    "device_type = 'cpu' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
    "ptdtype = {'float32': torch.float32, 'float64': torch.float64, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
    "\n",
    "load_meta=True\n",
    "meta_path='data/ukb/meta.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30b429fb-505d-47b3-8b56-4a2ba6fe7d01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Padding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Healthy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BMI_low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1265</th>\n",
       "      <td>D46 Myelodysplastic syndromes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1266</th>\n",
       "      <td>D47 Other neoplasms of uncertain or unknown be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1267</th>\n",
       "      <td>D48 Neoplasm of uncertain or unknown behaviour...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1268</th>\n",
       "      <td>O01 Hydatidiform mole38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1269</th>\n",
       "      <td>Death</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1270 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      0\n",
       "0                                               Padding\n",
       "1                                               Healthy\n",
       "2                                                Female\n",
       "3                                                  Male\n",
       "4                                               BMI_low\n",
       "...                                                 ...\n",
       "1265                      D46 Myelodysplastic syndromes\n",
       "1266  D47 Other neoplasms of uncertain or unknown be...\n",
       "1267  D48 Neoplasm of uncertain or unknown behaviour...\n",
       "1268                            O01 Hydatidiform mole38\n",
       "1269                                              Death\n",
       "\n",
       "[1270 rows x 1 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#labels = pd.read_csv(\"data/ukb/fields.txt\", header=None).merge(pd.read_csv(\"data/ukb/icd10_codes_mod.tsv\", sep='\\t',header=None, index_col=0), left_on=0, right_index=True)\n",
    "#labels[1] = labels[1].str.replace(\"Source of report of \",\"\")\n",
    "#train.max(0)\n",
    "labels = pd.read_csv(\"data/ukb/labels.csv\", header=None, sep=\"\\t\")\n",
    "labels_short = np.asarray(labels).astype('S3').astype(str)\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea807de4-84f7-4037-8752-6beb72b66be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: using slow attention. Flash Attention atm needs PyTorch nightly and dropout=0.0\n",
      "WARNING: using slow attention. Flash Attention atm needs PyTorch nightly and dropout=0.0\n",
      "WARNING: using slow attention. Flash Attention atm needs PyTorch nightly and dropout=0.0\n",
      "WARNING: using slow attention. Flash Attention atm needs PyTorch nightly and dropout=0.0\n",
      "WARNING: using slow attention. Flash Attention atm needs PyTorch nightly and dropout=0.0\n",
      "WARNING: using slow attention. Flash Attention atm needs PyTorch nightly and dropout=0.0\n",
      "WARNING: using slow attention. Flash Attention atm needs PyTorch nightly and dropout=0.0\n",
      "WARNING: using slow attention. Flash Attention atm needs PyTorch nightly and dropout=0.0\n",
      "WARNING: using slow attention. Flash Attention atm needs PyTorch nightly and dropout=0.0\n",
      "WARNING: using slow attention. Flash Attention atm needs PyTorch nightly and dropout=0.0\n",
      "WARNING: using slow attention. Flash Attention atm needs PyTorch nightly and dropout=0.0\n",
      "WARNING: using slow attention. Flash Attention atm needs PyTorch nightly and dropout=0.0\n",
      "number of parameters: 2.24M\n",
      "Loading meta from data/ukb/meta.pkl...\n",
      "1270\n",
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'n_layer': 12,\n",
       " 'n_head': 12,\n",
       " 'n_embd': 120,\n",
       " 'block_size': 48,\n",
       " 'bias': False,\n",
       " 'vocab_size': 1270,\n",
       " 'dropout': 0.0,\n",
       " 'token_dropout': 0.0,\n",
       " 't_min': 0.1,\n",
       " 'mask_ties': True,\n",
       " 'ignore_tokens': [0, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model\n",
    "if init_from == 'resume':\n",
    "    # init from a model saved in a specific directory\n",
    "    ckpt_path = os.path.join(out_dir, 'ckpt.pt')\n",
    "    checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "    #checkpoint['model_args']['token_dropout'] = 1.0\n",
    "    gptconf = GPTConfig(**checkpoint['model_args'])\n",
    "    model = GPT(gptconf)\n",
    "    state_dict = checkpoint['model']\n",
    "    unwanted_prefix = '_orig_mod.'\n",
    "    for k,v in list(state_dict.items()):\n",
    "        if k.startswith(unwanted_prefix):\n",
    "            state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "    model.load_state_dict(state_dict)\n",
    "elif init_from.startswith('gpt2'):\n",
    "    # init from a given GPT-2 model\n",
    "    model = GPT.from_pretrained(init_from, dict(dropout=0.0))\n",
    "\n",
    "model.eval()\n",
    "model.to(device)\n",
    "if compile:\n",
    "    model = torch.compile(model) # requires PyTorch 2.0 (optional)\n",
    "\n",
    "# look for the meta pickle in case it is available in the dataset folder\n",
    "if load_meta:\n",
    "    print(f\"Loading meta from {meta_path}...\")\n",
    "    with open(meta_path, 'rb') as f:\n",
    "        meta = pickle.load(f)\n",
    "    # TODO want to make this more general to arbitrary encoder/decoder schemes\n",
    "    stoi, itos = meta['stoi'], meta['itos']\n",
    "    #print(stoi, itos)\n",
    "    print(checkpoint['model_args']['vocab_size'])\n",
    "    token_length = int(np.log(checkpoint['model_args']['vocab_size'])/np.log(meta['vocab_size']))\n",
    "    print(token_length)\n",
    "    encode = lambda s: [stoi[c] for c in s]\n",
    "    decode = lambda l: ''.join([itos[int(j)] for i in l for j in np.base_repr(i,meta['vocab_size'], token_length)[-token_length:][::-1]])\n",
    "else:\n",
    "    # ok let's assume gpt-2 encodings by default\n",
    "    print(\"No meta.pkl found, assuming GPT-2 encodings...\")\n",
    "    enc = tiktoken.get_encoding(\"gpt2\")\n",
    "    encode = lambda s: enc.encode(s, allowed_special={\"<|endoftext|>\"})\n",
    "    decode = lambda l: enc.decode(l)\n",
    "checkpoint['model_args']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59140ea1-1c60-4e26-85ad-687120d5bcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_p2i(data):\n",
    "    px = data[:,0].astype('int')\n",
    "    pix = sorted(list(set(px)))\n",
    "    #p2i = np.array([(p, (px==p).argmax(), (px==p).sum()) for i,p in enumerate(pix)])\n",
    "    p2i = []\n",
    "    j = 0\n",
    "    q = px[0]\n",
    "    for i,p in enumerate(px):\n",
    "        if p != q:\n",
    "            p2i.append([j,i-j])\n",
    "            q = p\n",
    "            j = i\n",
    "    return np.array(p2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a099c88-02d0-4912-9a8c-009e269661c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = np.fromfile('/nfs/research/sds/sds-ukb-cancer/projects/gpt/data/pre.bin', dtype=np.uint32, ).reshape(-1,3)\n",
    "pre_p2i = get_p2i(pre)\n",
    "pre[:, -1] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4d96827-7d17-40c1-b844-ef8eec4cd25b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 471057/471057 [44:33<00:00, 176.18it/s]\n"
     ]
    }
   ],
   "source": [
    "nextlogits = []\n",
    "padding='random'\n",
    "with torch.no_grad():\n",
    "    for ii in tqdm(range(pre_p2i.shape[0])):\n",
    "        x = torch.from_numpy((pre[pre_p2i[ii, 0]:pre_p2i[ii, 0]+pre_p2i[ii, 1], 2][None, :]).astype(int)).to(device)\n",
    "        a = torch.from_numpy((pre[pre_p2i[ii, 0]:pre_p2i[ii, 0]+pre_p2i[ii, 1], 1][None, :]).astype(int)).to(device)\n",
    "        if padding == 'regular':\n",
    "            pad = torch.arange(3652.5/2, 36525, 3652.5/2) * torch.ones(1,1)\n",
    "        else:\n",
    "            pad = torch.randint(36525, (1, 20)) + 1\n",
    "\n",
    "        pad = pad.to(device)\n",
    "        m = a.max()\n",
    "        x = torch.hstack([x, torch.ones(1, pad.shape[1], dtype=torch.int).to(device)])\n",
    "        a = torch.hstack([a, pad])\n",
    "        \n",
    "        s = torch.argsort(a, 1)\n",
    "        x = torch.gather(x,1,s)\n",
    "        a = torch.gather(a,1,s)\n",
    "        a = a.type(torch.int32)\n",
    "        idx = a <= m \n",
    "        a = a[idx][None, :]\n",
    "        x = x[idx][None, :]\n",
    "        nextlogits.extend(model(x, a)[0][:, -1, :].cpu().numpy().tolist())\n",
    "nextlogits = np.asarray(nextlogits).astype(np.float32)\n",
    "nextlogits.tofile(f'/nfs/research/sds/sds-ukb-cancer/projects/gpt/out/nextlogits.bin')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11d4771-1e84-46b5-9a73-9a62a46c6241",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|███████▉                      | 124974/471057 [5:15:54<15:11:32,  6.33it/s]"
     ]
    }
   ],
   "source": [
    "trajectory = []\n",
    "padding='random'\n",
    "nsamples=5\n",
    "with torch.no_grad():\n",
    "    for ii in tqdm(range(pre_p2i.shape[0])):\n",
    "        ll = []\n",
    "        for _ in range(5):\n",
    "            x = torch.from_numpy((pre[pre_p2i[ii, 0]:pre_p2i[ii, 0]+pre_p2i[ii, 1], 2][None, :]).astype(int)).to(device)\n",
    "            a = torch.from_numpy((pre[pre_p2i[ii, 0]:pre_p2i[ii, 0]+pre_p2i[ii, 1], 1][None, :]).astype(int)).to(device)\n",
    "            if padding == 'regular':\n",
    "                pad = torch.arange(3652.5/2, 36525, 3652.5/2) * torch.ones(1,1)\n",
    "            else:\n",
    "                pad = torch.randint(36525, (1, 20)) + 1\n",
    "    \n",
    "            pad = pad.to(device)\n",
    "            m = a.max()\n",
    "            x = torch.hstack([x, torch.ones(1, pad.shape[1], dtype=torch.int).to(device)])\n",
    "            a = torch.hstack([a, pad])\n",
    "            \n",
    "            s = torch.argsort(a, 1)\n",
    "            x = torch.gather(x,1,s)\n",
    "            a = torch.gather(a,1,s)\n",
    "            a = a.type(torch.int32)\n",
    "            idx = a <= m \n",
    "            a = a[idx][None, :]\n",
    "            x = x[idx][None, :]\n",
    "            age = a[-1,-1]\n",
    "            rr = []\n",
    "            x_, y_, logits = model.generate(idx=x, age=a, max_new_tokens=10, max_age=age+900, temperature=1.0, no_repeat=True) #3.2*365.25\n",
    "            logits = logits.cpu().numpy()\n",
    "            logits = logits[0, x.shape[1]-1:, :]\n",
    "        \n",
    "            idxinf = logits[0, :] == -np.inf\n",
    "            logits[logits == -np.inf] = np.nan\n",
    "            logits[:, idxinf] = -np.inf\n",
    "            ll.extend(np.nanmean(logits, axis=0)[None, :].tolist())\n",
    "        trajectory.extend(np.mean(ll, axis=0)[None, :].tolist())\n",
    "trajectory = np.asarray(trajectory).astype(np.float32)\n",
    "trajectory.tofile(f'/nfs/research/sds/sds-ukb-cancer/projects/gpt/out/trajectory.bin')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7301abfc-ba84-43b1-99ed-7fe452bd8436",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4555863f-a8cc-479f-b405-cda84f77c92c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c83636-922a-432e-b5d8-9d7f09a21921",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "bkill 76298994"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "61b4d74e-d408-434d-928e-a43dee1c3fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JOBID      USER    STAT  QUEUE      FROM_HOST   EXEC_HOST   JOB_NAME   SUBMIT_TIME\n",
      "76298994   alexwju RUN   gpu-a100   codon-login codon-gpu-0 */bin/bash Jan 15 07:59\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "bjobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc54965c-c465-4048-940d-fbcdd2e58ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run:\n",
    "    next_probs = []\n",
    "    with torch.no_grad():\n",
    "        for ii in tqdm(range(pre_p2i.shape[0])):\n",
    "            x = torch.from_numpy((pre[pre_p2i[ii, 0]:pre_p2i[ii, 0]+pre_p2i[ii, 1], 2][None, :]).astype(int)).to(device)\n",
    "            a = torch.from_numpy((pre[pre_p2i[ii, 0]:pre_p2i[ii, 0]+pre_p2i[ii, 1], 1][None, :]).astype(int)).to(device)\n",
    "            # next token probs\n",
    "            nprobs = F.softmax(model(x, a)[0][:, -1, :], dim=-1)\n",
    "            nprobs = nprobs.cpu().numpy()\n",
    "            next_probs.extend(nprobs.tolist())     \n",
    "    next_probs = np.asarray(next_probs).astype(np.float32)\n",
    "    next_probs.tofile(f'/nfs/research/sds/sds-ukb-cancer/projects/gpt/out/next_probs.bin')\n",
    "else:\n",
    "    next_probs = np.fromfile(f'/nfs/research/sds/sds-ukb-cancer/projects/gpt/out/next_probs.bin', dtype=np.float32).reshape(-1, 1270)\n",
    "\n",
    "\n",
    "traj = []\n",
    "with torch.no_grad():\n",
    "    for ii in tqdm(range(pre_p2i.shape[0])):\n",
    "        x = (pre[pre_p2i[ii, 0]:pre_p2i[ii, 0]+pre_p2i[ii, 1], 2][None, :]).astype(int)\n",
    "        a = (pre[pre_p2i[ii, 0]:pre_p2i[ii, 0]+pre_p2i[ii, 1], 1][None, :]).astype(int)\n",
    "        age = a[-1,-1]\n",
    "        rr = []\n",
    "        for i in range(5):\n",
    "            x_, y_, logits = model.generate(idx=torch.from_numpy(x).to(device), age=torch.from_numpy(a).to(device), max_new_tokens=10, max_age=age+900, temperature=1.0, no_repeat=True) #3.2*365.25\n",
    "            logits = logits.cpu().numpy()\n",
    "            r = 1 / (1 + np.exp(-logits[0,:,:])/365.25)\n",
    "            r = r.max(axis=0)[None, :]\n",
    "            rr.extend(r.tolist())\n",
    "        rr = np.median(np.asarray(rr), axis=0)[None, :].tolist()\n",
    "        traj.extend(rr)\n",
    "traj = np.asarray(traj).astype(np.float32)\n",
    "traj.tofile(f'/nfs/research/sds/sds-ukb-cancer/projects/gpt/out/trajectory3.bin')\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
